{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory for video data\n",
    "#DATA_DIR = r\"C:\\Users\\knila\\Few Shot video classification\\Train\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gather video paths\n",
    "video_paths = list(Path(DATA_DIR).rglob(\"*.mp4\"))\n",
    "train_video_paths, test_video_paths = train_test_split(video_paths, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Labeling function\n",
    "def labeling(vid_paths):\n",
    "    return [path.parent.name for path in vid_paths]\n",
    "\n",
    "train_labels = labeling(train_video_paths)\n",
    "test_labels = labeling(test_video_paths)\n",
    "unique_labels = sorted(set(train_labels))\n",
    "label_dict = {label: i for i, label in enumerate(unique_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.video loader\n",
    "def video_to_tensor(video_path, expected_frames=32, height=112, width=112):\n",
    "    capture = cv.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    for _ in range(expected_frames):\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv.resize(frame, (width, height), interpolation=cv.INTER_AREA)\n",
    "        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        frame = torch.from_numpy(frame)\n",
    "        frames.append(frame)\n",
    "    capture.release()\n",
    "    if len(frames) == 0:\n",
    "        # Return a zero tensor if video is unreadable\n",
    "        return torch.zeros(3, expected_frames, height, width)\n",
    "    frames = torch.stack(frames)\n",
    "    frames = frames.float() / 255.0\n",
    "    frames = frames.permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "    # Pad if not enough frames\n",
    "    if frames.shape[1] < expected_frames:\n",
    "        pad = expected_frames - frames.shape[1]\n",
    "        frames = F.pad(frames, (0, 0, 0, 0, 0, pad))\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pair dataset with deterministic sampling\n",
    "class PairVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, pairs_per_epoch=1000):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.pairs = []\n",
    "        # Precompute pairs for reproducibility and efficiency\n",
    "        label_to_indices = {label: [i for i, l in enumerate(labels) if l == label] for label in set(labels)}\n",
    "        for _ in range(pairs_per_epoch):\n",
    "            idx1 = random.randint(0, len(video_paths) - 1)\n",
    "            label1 = labels[idx1]\n",
    "            # Similar pair\n",
    "            idx2 = random.choice(label_to_indices[label1])\n",
    "            self.pairs.append((idx1, idx2, 1))\n",
    "            # Dissimilar pair\n",
    "            label2 = random.choice([l for l in set(labels) if l != label1])\n",
    "            idx3 = random.choice(label_to_indices[label2])\n",
    "            self.pairs.append((idx1, idx3, 0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, sim = self.pairs[idx]\n",
    "        video1 = video_to_tensor(self.video_paths[idx1])\n",
    "        video2 = video_to_tensor(self.video_paths[idx2])\n",
    "        return video1, video2, torch.tensor(sim, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Siamese network (single backbone)\n",
    "class SiamesePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.video.r3d_18(weights='DEFAULT')\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze last two layers\n",
    "        for child in list(self.backbone.children())[-2:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(num_features, 512)\n",
    "        self.fc = nn.Linear(512, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "model = SiamesePredictor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Contrastive loss (vectorized)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        cos_sim = F.cosine_similarity(output1, output2)\n",
    "        loss = torch.mean((1 - label) * (1 - cos_sim) + label * torch.clamp(cos_sim - self.margin, min=0.0))\n",
    "        return loss\n",
    "\n",
    "loss_fn = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. DataLoader\n",
    "train_dataset = PairVideoDataset(train_video_paths, train_labels, pairs_per_epoch=500)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_dataset = PairVideoDataset(test_video_paths, test_labels, pairs_per_epoch=100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.fc.parameters()) +\n",
    "    list(list(model.backbone.children())[-2].parameters()) +\n",
    "    list(list(model.backbone.children())[-1].parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for video1, video2, sim_label in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        video1 = video1.to(device)\n",
    "        video2 = video2.to(device)\n",
    "        sim_label = sim_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model(video1)\n",
    "        out2 = model(video2)\n",
    "        loss = loss_fn(out1, out2, sim_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for video1, video2, sim_label in test_loader:\n",
    "        video1 = video1.to(device)\n",
    "        video2 = video2.to(device)\n",
    "        out1 = model(video1)\n",
    "        out2 = model(video2)\n",
    "        cos_sim = F.cosine_similarity(out1, out2)\n",
    "        pred = (cos_sim > 0.8).int().cpu().numpy()\n",
    "        predictions.extend(pred.tolist())\n",
    "        true_labels.extend(sim_label.int().cpu().numpy().tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
